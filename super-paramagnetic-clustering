import numpy as np
import matplotlib.pyplot as plt
from joblib import Parallel, delayed
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn import datasets
import umap
from sklearn import metrics
from numpy import linalg as la
import networkx as nx
from scipy.stats import itemfreq
import matplotlib as mpl
import os


def duplicates(lst, item):
    return [i for i, x in enumerate(lst) if x == item]

def nearestPD(A):
    """Find the nearest positive-definite matrix to input
    """

    B = (A + A.T) / 2
    _, s, V = la.svd(B)

    H = np.dot(V.T, np.dot(np.diag(s), V))

    A2 = (B + H) / 2

    A3 = (A2 + A2.T) / 2

    if isPD(A3):
        return A3

    spacing = np.spacing(la.norm(A))
    # The above is different from [1]. It appears that MATLAB's `chol` Cholesky
    # decomposition will accept matrixes with exactly 0-eigenvalue, whereas
    # Numpy's will not. So where [1] uses `eps(mineig)` (where `eps` is Matlab
    # for `np.spacing`), we use the above definition. CAVEAT: our `spacing`
    # will be much larger than [1]'s `eps(mineig)`, since `mineig` is usually on
    # the order of 1e-16, and `eps(1e-16)` is on the order of 1e-34, whereas
    # `spacing` will, for Gaussian random matrixes of small dimension, be on
    # othe order of 1e-16. In practice, both ways converge, as the unit test
    # below suggests.
    I = np.eye(A.shape[0])
    k = 1
    while not isPD(A3):
        mineig = np.min(np.real(la.eigvals(A3)))
        A3 += I * (-mineig * k**2 + spacing)
        k += 1

    return A3
def isPD(B):
    """Returns true when input is positive-definite, via Cholesky"""
    try:
        _ = la.cholesky(B)
        return True
    except la.LinAlgError:
        return False

def eHKnn(distance,K):
    N=distance.shape[0]
    nodenext=[]
    Rank=np.argsort(distance)
    Rank=Rank[:,:K+1]
    r_ = np.zeros( (N,K), dtype=int)
    for i in range(N):
        rank = list(Rank[i])
        rank.remove(i)
        r_[i] = rank[:K]
#        try: r_[i] = np.array( [x for x in Rank[i] if x!=i])
#        except: r_[i] = Rank[i,:K]
    Rank =  r_
    for i in range(N):
        e=[]
        for j in Rank[i]:
            if i in Rank[j]:
#            if (Rank[i,:]==j).any() and (Rank[j,:]==i).any():
                e.append(j)
        nodenext.append(e)
    return nodenext

def eHK(link,nodenext):

    N=len(nodenext)
    nodel=10*N*np.ones(N,dtype=int)
    label_counter=0
    nodelp=np.array([],dtype=int)
    for i in range(N):
        if (np.array(link[i])==0).all():
            nodel[i]=label_counter
            nodelp=np.append(nodelp,label_counter)
            label_counter+=1
        else:
            idx = duplicates(link[i],1) #where links are
            t=[nodel[ nodenext[i][j] ] for j in idx]
            t=np.array(t)
            if (t==10*N).all(): # all unlabeled?
                nodel[i]=label_counter
                nodelp=np.append(nodelp,label_counter)
                label_counter+=1
            else:
                w=[]
                for index in range(len(t)):
                    if t[index]!=10*N:
                        w.append(index)
                idx_ = np.array([nodenext[i][j] for j in idx])
                z = nodelp[nodel[ idx_[w] ] ]
                min_ = np.amin(z)

                nodel[i] = min_
                a = nodel[ idx_[w] ]
                nodelp[a]=min_

    # sequentialize part 1: re-order nodelp
    for y in range(len(nodelp)):
        n = y
        while (nodelp[n]<n):
            n=nodelp[n]
        nodelp[y]=n
    # sequentialize part 2: get rid of the gaps
    un = np.unique(nodelp)
    for i in range(len(un)-1):
        while un[i+1]-un[i] !=1:
            idx = np.where(nodelp==un[i+1])[0]
            nodelp[idx] -= 1
            un = np.unique(nodelp)

    # rename the labels with their root
    for i in range( len(nodelp) ):
        nodel[nodel==i]=nodelp[i]

    return nodel



def cHKlons(nodenext,G):
    link=[]
    N=G.shape[0]
    for i in range(N):
        neighbors=nodenext[i]
        e=[]
        for j in neighbors:
            if (G[i,j]>0.5):
                e.append(1)
            else:
                e.append(0)
        idx=np.argmax(G[i,neighbors])
        e[idx]=1
        link.append(e)
    ''' make sure the neighbor with the highest correlation is linked both ways'''
    for i in range(N):
        idx = duplicates(link[i],1)
        for f in idx:
            X = nodenext[i][f]
            Y = duplicates(nodenext[X],i)
            Y = Y[0]
            link[X][Y] = 1
    return link

def pclus(nodenext, G):
    link = cHKlons(nodenext,G)
    cluster= eHK(link,nodenext)
    return cluster, link

def sclus(cluster,nbr_clus):
    stats = itemfreq(cluster)
    idx = np.argsort(stats[:,1])
    stats = stats[idx]
    res = list(stats[len(stats)-nbr_clus:,1])
    while len(res)!=nbr_clus:
        res.insert(0,0)
    return res

def kron(i, j):
    if i == j:
        return 1
    else:
        return 0

def twopc(S,cij):
    ''' two point connectedness'''
    classes=np.unique(S)
    for label in classes:
        neighbors=duplicates(S,label)
        for node in neighbors:
            cij[node,neighbors]+=1
    return cij



def Hs(S, J, nodenext):
    E=0
    N = len(S)
    for i in range(N):
        for j in nodenext[i]:
            E += J[i, j]*(1-kron(S[i], S[j]))
    return E/N

def magnetization(S, q):
    N=len(S)
    nmax = np.amax(np.bincount(S))  # find frequency count and datak highest
    return (q*nmax-N)/((q-1)*N)

def runz(S,f,mcmc,nodenext,J,t,q,K):
    np.random.seed(0)
    def flip(S, q):
        c = np.unique(S)  # find unique labels
        new_c = np.random.randint(0, q, len(c))  #gen new spins for clusters
        conv = dict(zip(c, new_c))  # use dic to assign new spins to clusters
        return np.vectorize(conv.get)(S)

    def eHKlons(nodenext,T,J,S):
        link=[]
        N = len(nodenext)
        for i in range(N):
            e=[]
            for j in nodenext[i]:
                if (1-np.exp(-J[i,j]*kron(S[i],S[j])/T)>np.random.uniform() ):
                    e.append(1)
                else:
                    e.append(0)
            link.append(e)
        return link

    N=len(S)
    forget=int(f*mcmc)
    m=np.zeros(mcmc)
    cij=np.zeros((N,N))
    for i in range(forget):
        S1=S
        E1=Hs(S1,J,nodenext)
        LinkSnode = eHKlons(nodenext,t,J,S)
        S = eHK(LinkSnode,nodenext)
        S = flip(S, q)
        E2 = Hs(S,J,nodenext)
        if E2 >= E1:
            if np.exp(- E2 / t) < np.random.uniform() :
                S=S1
    for i in range(mcmc):
        S1=S
        E1=Hs(S1,J,nodenext)
        LinkSnode = eHKlons(nodenext,t,J,S)
        S = eHK(LinkSnode,nodenext)
        S = flip(S, q)
        E2 = Hs(S,J,nodenext)
        if E2 >= E1:
            if np.exp(- E2 / t) < np.random.uniform() :
                S=S1
                E2 = E1

        cij=twopc(S,cij)
        m[i]=magnetization(S,q)
    mbar = np.average(m)
#    su = np.var(m)
    su = N*np.var(m)/t
    return su, mbar, cij, S




project = 'sp500'
subproject = 'data'
cor = None
mode = None
bag = ['sp500','zillow','bls','usstocks']


if project == 'iris':
#    import pandas as pd
#    iris = pd.read_csv('data/iris.csv')
#    data = iris[iris.columns[0:4]]
#    key = iris['class']
#    data = np.array(data)

    iris = datasets.load_iris()
    data=iris.data
    key = iris.target
    name = key
    data = MinMaxScaler().fit_transform(data)
    N = data.shape[0]

    T=np.linspace(1e-6,.3,num=60,endpoint=True)
    K = 7
    q = 20
    alpha = 2
    nbr_clus = 3
    distance = euclidean_distances(data)
    D = np.zeros( (data.shape[1],N,N))
    for i in range(data.shape[1]):
        D[i,:,:] = euclidean_distances(data[:,i].reshape(-1,1))
    for i in range(N):
        for j in range(N):
            distance[i,j] = np.sum(D[:,i,j])
    del D
elif project == 'blobs':
    blob = datasets.make_blobs(n_samples=500,
                             cluster_std=[0.25,0.5,1],
                             random_state=0, n_features=500,shuffle=False)
    data = blob[0]
    data = MinMaxScaler().fit_transform(data)
#    data = StandardScaler().fit_transform(data)

    key = blob[1]
    name = key
    N = data.shape[0]
    T=np.linspace(1e-6,.3,num=60,endpoint=True)
    K = 10
    q = 20
    alpha = 4
    nbr_clus = 3

    distance = euclidean_distances(data)
elif project == 'circle':
    circle = datasets.make_circles(n_samples=500, factor=0.1,
                                      noise=.05, random_state=0, shuffle=False)
    data = circle[0]
    data = MinMaxScaler().fit_transform(data)

    key = circle[1]
    name = key
    N = data.shape[0]
    T = np.linspace(1e-6,.3,num=60,endpoint=True)
    K = 10
    q = 20
    alpha = 4
    nbr_clus = 2

    distance = euclidean_distances(data)
elif project =='digits':
    digits = datasets.load_digits()
    data=digits.data
    N = data.shape[0]
    key = digits.target
    name = key
    ''' sample for 500 observations'''
#    numbers = np.unique(key)
#    f_500 = []
#    for i in numbers:
#        idx = duplicates(key, i)
#        pick = np.random.choice(idx,size=50,replace=False)
#        f_500.extend(pick)
#    np.save('data/%s/f_500.npy' % project, f_500)
    f_500 = np.load('data/%s/f_500.npy' % project)
    data = data[f_500]
    key = key[f_500]
    name = name[f_500]
    N = data.shape[0]
    T=np.linspace(1e-6,.3,num=60,endpoint=True)
    K = 10
    q = 20
    alpha = 5
    nbr_clus = 10
    data = MinMaxScaler().fit_transform(data)

    distance = euclidean_distances(data)
elif project =='wine':
    wine = datasets.load_wine()
    data = wine.data
    key = wine.target
    name = key
    N = data.shape[0]
    data = MinMaxScaler().fit_transform(data)
    T=np.linspace(1e-6,.3,num=60,endpoint=True)
    K = 10
    q = 20
    alpha = 2
    nbr_clus = 3

    '''wishart'''
    cor = np.corrcoef(data)
    cor = nearestPD(cor)
    eig_vals, eig_vecs = np.linalg.eigh(cor)
    eig_pairs = [ (np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]
    eig_pairs.sort()
    Q = data.shape[1]/data.shape[0]
    lamax = 1+1/Q+2*np.sqrt(1/Q)
    lamin = 1+1/Q-2*np.sqrt(1/Q)
    wishart_pairs = [ i for i in eig_pairs if i[0] < lamin or i[0]> lamax ]
#    wishart_pairs = [ i for i in eig_pairs if i[0]> lamax ]

    W = np.hstack( (i[1].reshape(data.shape[0],1) for i in wishart_pairs) )
    Z = np.dot(W.T,data)
    data = np.dot(W,Z)


    distance = euclidean_distances(data)

elif project == 'sp500':
    import pandas as pd
    subproject = 'sp500_1250d'
    data = pd.read_csv('data/%s.csv' % subproject, index_col ='Name')
    name = np.array((data.index))
    ''' Industry and Sector Coloring'''
#    keys = pd.read_csv('data/key_s500_vol.csv', index_col ='Name')
    keys = pd.read_csv('data/key_stock.csv', index_col ='Name')
    key = keys['Sector']
    data = np.diff(np.log(data))
    N = data.shape[0]

    from scipy.spatial.distance import pdist, squareform
    dmat = squareform(pdist(data, 'correlation'))
    order = np.argsort(dmat[0,:])
    data = data[order]
    key = key[order]
    name = name[order]

    mode = 'rmt'
    if not os.path.exists('data/%s/%s/%s' % (project,subproject,mode)):
        os.makedirs('data/%s/%s/%s' % (project,subproject,mode))
    if mode == 'rmt':
        ''' Use wishart method with pca to remove principal components'''
        cor = np.corrcoef(data)
        cor = nearestPD(cor)
        eig_vals, eig_vecs = np.linalg.eigh(cor)
        eig_pairs = [ (np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]
#        eig_pairs = list(zip (eig_vals, eig_vecs) ) bad bad bad
        eig_pairs.sort()
        Q = data.shape[1]/data.shape[0]
        lamax = 1+1/Q+2*np.sqrt(1/Q)
        lamin = 1+1/Q-2*np.sqrt(1/Q)
        
        
        

        #figure for the barplot of the eigenvalues distribution
#        wishart_pairs = [ i for i in eig_pairs if i[0] >= lamin and i[0]<= lamax ]
#        fig = plt.figure(figsize=(4,4),dpi=300)
#        ax0 = fig.add_subplot(111)
#        num_bins = int(data.shape[1]/2)
#        n, bins, patches = ax0.hist(eig_vals, num_bins, density='normed', facecolor='b')
##        ax0.set_xscale('log')
#        ax0.axes.axvline(x=lamin, c='red')
#        ax0.axes.axvline(x=lamax, c='red')
#        ax0.set_ylim((0,0.8))
#        ax0.set_xlim((0,5))
#        plt.xlabel('eigenvalues')
#        plt.ylabel('Probability')
#        # inset for the wishart range distribution
#        wishart_eig = np.array( [ i[0] for i in wishart_pairs ] )
#        plt.axes([0.4, 0.4, .45, .45])
#        pdfla = (Q/(2*np.pi*wishart_eig)) * np.sqrt((lamax-wishart_eig)*(wishart_eig-lamin))
##        plt.figure()
#        plt.plot(wishart_eig,pdfla, c='r'), plt.xlabel(r'$\lambda$'), plt.ylabel(r'$p(\lambda)$')
##        plt.scatter(wishart_eig,pdfla, c='red', s=3), plt.tight_layout()
#        plt.savefig('data/%s/wishart_pdf_%s.png' % (project,data.shape[1]) )
#        plt.close()
        ''' sort the eigenvalues, and pick the ones outside the wishart range'''
        
        eig_vals.sort()
        idx = ( eig_vals > lamin) & (eig_vals < lamax)
        idx = np.invert(idx)
#        idx = (eig_vals > lamax) # only biggest principal components
        eig_pairs = np.array( eig_pairs, dtype=object)
        wishart_pairs = eig_pairs[idx]

        W = np.hstack( (i[1].reshape(data.shape[0],1) for i in wishart_pairs) )

        Z = np.dot(W.T,data)
        data = np.dot(W,Z)
        cor = np.corrcoef(data)
        
        cor = nearestPD(cor)
        cor[cor>1]=1


        K = 10

    elif mode == 'pca':
        ''' Use pca to remove principal components'''
        cor = np.cov(data)
        #
        eig_vals, eig_vecs = np.linalg.eigh(cor)
        eig_pairs = [ (np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]
        eig_pairs.sort()
        eig_pairs.reverse()

        tot = sum(np.abs(eig_vals))
        var_exp = [ (i/tot)*100 for i in sorted(np.abs(eig_vals), reverse=True)]
        #
        idx = list( range(2,5) )


        W = np.hstack( (eig_pairs[i][1].reshape(data.shape[0],1) for i in idx) )
        #
        Z = np.dot(W.T,data)
        #
        data = np.dot(W,Z)
        cor = np.corrcoef(data)
    elif mode == 'norm':
        ''' remove the market mode by successive normalization'''
        cov = np.cov(data)
        for i in range(500):
            cov = StandardScaler().fit_transform(cov)
            cov = cov.T

        cor= np.zeros((N,N))
        for i in range(N):
            for j in range(N):
                cor[i,j] = cov[i,j]/(np.sqrt(cov[i,i])*np.sqrt(cov[j,j]))
        cor = nearestPD(cor)
        cor[cor>1] = 1
        K = 10
    elif mode == 'full':
        cor = np.corrcoef(data)
        K = 5
    ''' convert correlation to distance'''
    distance = 1-cor
    T=np.linspace(1e-6,.2,num=60,endpoint=True)
    q = 100
    alpha = 1
    nbr_clus = 10
    # code bins correlation matrices, repeat it and change the mode
#    plt.figure(figsize=(4,4),dpi=300)
#    histogram = plt.hist(cor, bins=20, facecolor='red')
#    plt.tight_layout()
#    plt.savefig('data/%s/%s_%s_%s_cor_bins.png' % (project,project,subproject,mode) )
#    plt.close()

elif project == 'zillow':
    import pandas as pd
    subproject = 'state'
    data = pd.read_csv('data/z_%s.csv' % subproject, index_col ='RegionName')
    name = np.array((data.index))
    ''' Coloring'''
#    key = pd.read_csv('data/key_house.csv', index_col ='RegionName')
    key = pd.read_csv('data/regx_%s.csv' % subproject, index_col ='RegionName')
#    key = pd.read_csv('data/bear_%s.csv' % subproject, index_col ='RegionName')
    key = key['Name']
    data = np.diff(np.log(data))
    N = data.shape[0]

    from scipy.spatial.distance import pdist, squareform
    dmat = squareform(pdist(data, 'correlation'))
    order = np.argsort(dmat[0,:])
    data = data[order]
    key = key[order]
    name = name[order]

    mode = 'full'
    if not os.path.exists('data/%s/%s/%s' % (project,subproject,mode)):
        os.makedirs('data/%s/%s/%s' % (project,subproject,mode))
    if mode == 'rmt':
        ''' Use wishart method with pca to remove principal components'''
        cor = np.corrcoef(data)
        cor = nearestPD(cor)
        eig_vals, eig_vecs = np.linalg.eigh(cor)
        eig_pairs = [ (np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]
        eig_pairs.sort()
        Q = data.shape[1]/data.shape[0]
        lamax = 1+1/Q+2*np.sqrt(1/Q)
        lamin = 1+1/Q-2*np.sqrt(1/Q)
        wishart_pairs = [ i for i in eig_pairs if i[0] >= lamin and i[0]<= lamax ]

#        fig = plt.figure()
#        ax0 = fig.add_subplot(111)
#        num_bins = int(data.shape[1]/2)
#        n, bins, patches = ax0.hist(eig_vals, num_bins, normed=1, alpha=0.5)
#        ax0.axes.axvline(x=lamin, c='red')
#        ax0.axes.axvline(x=lamax, c='red')
#        plt.xlabel('eigenvalues')
#        plt.ylabel('Probability')
#        plt.tight_layout()
#        plt.savefig('data/%s/eigen_dist_%sD_%s_%s.png' % (project,data.shape[1], len(wishart_pairs),N ) )
#        plt.close()


#        wishart_eig = np.array( [ i[0] for i in wishart_pairs ] )
#        pdfla = (Q/(2*np.pi*wishart_eig)) * np.sqrt((lamax-wishart_eig)*(wishart_eig-lamin))
#        plt.figure()
#        plt.plot(wishart_eig,pdfla), plt.xlabel(r'$\lambda$'), plt.ylabel(r'$p(\lambda)$')
#        plt.scatter(wishart_eig,pdfla, c='red'), plt.tight_layout()
#        plt.savefig('data/%s/wishart_pdf_%s.png' % (project,data.shape[1]) )
#        plt.close()

        wishart_pairs = [ i for i in eig_pairs if i[0] < lamin or i[0]> lamax ]

        W = np.hstack( (i[1].reshape(data.shape[0],1) for i in wishart_pairs) )

        Z = np.dot(W.T,data)
        data = np.dot(W,Z)
        cor = np.corrcoef(data)
        cor = nearestPD(cor)
        cor[cor>1]=1

    elif mode == 'pca':
        ''' Use pca to remove principal components'''
        cor = np.cov(data)
        #
        eig_vals, eig_vecs = np.linalg.eigh(cor)
        eig_pairs = [ (np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]
        eig_pairs.sort()
        eig_pairs.reverse()

        tot = sum(np.abs(eig_vals))
        var_exp = [ (i/tot)*100 for i in sorted(np.abs(eig_vals), reverse=True)]
        #
        idx = list( range(2,5) )


        W = np.hstack( (eig_pairs[i][1].reshape(data.shape[0],1) for i in idx) )
        #
        Z = np.dot(W.T,data)
        #
        data = np.dot(W,Z)
        cor = np.corrcoef(data)
    elif mode == 'norm':
        ''' remove the market mode by successive normalization'''
        cov = np.cov(data)
        for i in range(500):
            cov = StandardScaler().fit_transform(cov)
            cov = cov.T

        cor= np.zeros((N,N))
        for i in range(N):
            for j in range(N):
                cor[i,j] = cov[i,j]/(np.sqrt(cov[i,i])*np.sqrt(cov[j,j]))
        cor = nearestPD(cor)
        cor[cor>1] = 1
    elif mode == 'full':
        cor = np.corrcoef(data)
    ''' convert correlation to distance'''
    distance = 1-cor
    T=np.linspace(1e-6,.4,num=60,endpoint=True)
    q = 20
    alpha = 1
    nbr_clus = 10
    K=3
elif project == 'bls':
    import pandas as pd
    subproject = 'metro'
#    subproject = 'county'
    data = pd.read_csv('data/bls_%s.csv' % subproject, index_col ='series_id')

    name = np.array((data.index))
    ''' Coloring'''
#    key = np.array(pd.read_csv('data/bls_regx.csv', index_col ='series_id'))
    key = np.array(pd.read_csv('data/bls_county_regx.csv', index_col ='series_id'))
    data = np.array(data)
    data[data==0] = 1e-6
    data = np.diff(np.log(data))
    D = data.shape[1]
    nn = 4
    data=data[:,D-(nn+1)*12:D-nn*12]
    N = data.shape[0]
    key = [ key[i][0] for i in range(N) ]

    from scipy.spatial.distance import pdist, squareform
    dmat = squareform(pdist(data, 'correlation'))
    order = np.argsort(dmat[0,:])
    data = data[order]
    key = key[order]
    name = name[order]

    mode = 'full'
    if not os.path.exists('data/%s/%s/%s' % (project,subproject,mode)):
        os.makedirs('data/%s/%s/%s' % (project,subproject,mode))
    if mode == 'rmt':
        ''' Use wishart method with pca to remove principal components'''
        cor = np.corrcoef(data)
        cor = nearestPD(cor)
        eig_vals, eig_vecs = np.linalg.eigh(cor)
        eig_pairs = [ (np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]
        eig_pairs.sort()
        Q = data.shape[1]/data.shape[0]
        lamax = 1+1/Q+2*np.sqrt(1/Q)
        lamin = 1+1/Q-2*np.sqrt(1/Q)
        wishart_pairs = [ i for i in eig_pairs if i[0] < lamin or i[0]> lamax ]

        W = np.hstack( (i[1].reshape(data.shape[0],1) for i in wishart_pairs) )

        Z = np.dot(W.T,data)
        data = np.dot(W,Z)
        cor = np.corrcoef(data)
        cor = nearestPD(cor)
        cor[cor>1]=1
    elif mode == 'norm':
        ''' remove the market mode by successive normalization'''
        cov = np.cov(data)
        for i in range(500):
            cov = StandardScaler().fit_transform(cov)
            cov = cov.T

        cor= np.zeros((N,N))
        for i in range(N):
            for j in range(N):
                cor[i,j] = cov[i,j]/(np.sqrt(cov[i,i])*np.sqrt(cov[j,j]))
        cor = nearestPD(cor)
        cor[cor>1] = 1
    elif mode == 'full':
        cor = np.corrcoef(data)
    ''' convert correlation to distance'''
    distance = 1-cor
    T=np.linspace(1e-6,.4,num=60,endpoint=True)
    q = 100
    alpha = 1
    nbr_clus = 10
    K=5
elif project == 'usstocks':
    import pandas as pd
    subproject = 'trading'
#    data = pd.read_csv('data/us_sec_250d.csv', index_col ='Name')
    data = pd.read_csv('data/us_intra_5m_sec_250d.csv', index_col ='Name')
#    data = pd.read_csv('data/us_intra_hr_sec_250d.csv', index_col ='Name')
    name = np.array((data.index))
    ''' Industry and Sector Coloring'''
#    key = np.load('data/key_us_sec.npy')
    key = np.load('data/key_intra_5m_us_sec.npy')
#    key = np.load('data/key_intra_hr_us_sec.npy')
    data = np.diff(np.log(data))
    N = data.shape[0]

    from scipy.spatial.distance import pdist, squareform
    dmat = squareform(pdist(data, 'correlation'))
    order = np.argsort(dmat[0,:])
    data = data[order]
    key = key[order]
    name = name[order]

    mode = 'rmt'
    if not os.path.exists('data/%s/%s/%s' % (project,subproject,mode)):
        os.makedirs('data/%s/%s/%s' % (project,subproject,mode))
    if mode == 'rmt':
        ''' Use wishart method with pca to remove principal components'''
        cor = np.corrcoef(data)
        eig_vals, eig_vecs = np.linalg.eigh(cor)
        eig_pairs = list(zip (eig_vals, eig_vecs) )
        eig_pairs.sort()
        Q = data.shape[1]/data.shape[0]
        lamax = 1+1/Q+2*np.sqrt(1/Q)
        lamin = 1+1/Q-2*np.sqrt(1/Q)
        eig_vals.sort()
        idx = ( eig_vals > lamin) & (eig_vals < lamax)
        idx = np.invert(idx)
        eig_pairs = np.array( eig_pairs, dtype=object)
        wishart_pairs = eig_pairs[idx]
        W = np.hstack( (i[1].reshape(data.shape[0],1) for i in wishart_pairs) )
        Z = np.dot(W.T,data)
        data = np.dot(W,Z)
        cor = np.corrcoef(data)
        K = 10
        del W
        del Z
        del eig_vals
        del eig_vecs
        del eig_pairs
    elif mode == 'norm':
        ''' remove the market mode by successive normalization'''
        cov = np.cov(data)
        for i in range(500):
            cov = StandardScaler().fit_transform(cov)
            cov = cov.T

        cor= np.zeros((N,N))
        for i in range(N):
            for j in range(N):
                cor[i,j] = cov[i,j]/(np.sqrt(cov[i,i])*np.sqrt(cov[j,j]))
        cor = nearestPD(cor)
        cor[cor>1] = 1
        K = 10
    elif mode == 'full':
        cor = np.corrcoef(data)
        K = 5
    ''' convert correlation to distance'''
    distance = 1-cor
    T=np.linspace(1e-6,.4,num=60,endpoint=True)
    q = 100
    alpha = 1
    nbr_clus = 10
    del dmat
    cor = 0


''' create folders'''

if not os.path.exists('data/%s/%s' % (project,subproject)):
    os.makedirs('data/%s/%s' % (project,subproject))

''' Number of mcmc steps'''
mcmc = 200
''' Number of temperatures explored'''
k = len(T)  # how many values we will record
'''determine the Graph, and its minimal spanning tree'''

''' Determine the neighborhood and add the Minimal Spanning Tree edges on top of it'''
from pathlib import Path

my_file = Path('data/%s/%s_%s_%s_graph.s6' % (project,project,subproject,mode))
if my_file.is_file():
    Tree =  nx.read_graph6('data/%s/%s_%s_%s_graph.s6' % (project,project,subproject,mode))
else:
    Tree=nx.minimum_spanning_tree(nx.from_numpy_matrix(distance))
    nx.write_graph6(Tree,'data/%s/%s_%s_%s_graph.s6' % (project,project,subproject,mode))

'''Save the mst as gephi file'''
#true_class = dict(zip(range(N), key.astype(str).tolist()))
#nx.set_node_attributes(Tree, name = 'true_class', values = true_class)
#nx.write_gexf(Tree,path = 'data/%s/mst_usstocks.gexf' % project)

nodenext = eHKnn(distance, K)
mst_edges = list( Tree.edges())
''' add the edges in the minimal spanning tree not in nodenext'''
for i in mst_edges:
    node = i[0]
    if i[1] not in nodenext[node]:
        nodenext[node].append(i[1])
        nodenext[node] = sorted(nodenext[node])
        nodenext[i[1]].append(node)
        nodenext[i[1]] =  sorted(nodenext[i[1]])

''' need average number of neighbors khat, and the local length scale a'''
khat = 0
for i in nodenext:
    khat+= len(i)
khat = khat / N

a = 0
for i in range(N):
    a+=sum(distance[i,nodenext[i]])
a = alpha * a / (khat*N)

''' Interaction Strength'''
n = 2
J = (1 / khat) * np.exp(-( (n-1)/n ) * ( distance / a)**n)

''' How many mcmc steps are forgotten for every temperature t'''
f_=0.5

''' The initial spin configuration S_0 for all temperatures'''
S=np.ones(N, dtype=int)
#embed = nx.nx_pydot.pydot_layout(Tree, prog='sfdp', overlap='compress')

embed = nx.nx_pydot.pydot_layout(Tree)

unique = np.unique( list(name) )
if project in bag:
    unique = np.unique (list(key))
plt.figure(figsize=(4,4),dpi=300)
ax0 = plt.subplot(111)
for i in range(len(unique)):
    idx = list( np.where(key==unique[i])[0] )
    labels = dict( (x, name[x]) for x in idx)
    nx.draw_networkx(Tree, pos=embed,
                    nodelist=idx,
                    font_size=1,
                    node_size = 10,
                    with_labels=False,
                    labels=labels,
                    node_color=plt.cm.tab20([i]),
                    label =  '%s' % unique[i],
                    ax=ax0, alpha = 0.75)

#from mpl_toolkits.axes_grid1 import make_axes_locatable
#h1, l1 = ax0.get_legend_handles_labels()
#divider = make_axes_locatable(ax0)
#cax = divider.append_axes("bottom", size="5%", pad=0.1)
#
#
#for i in range(1,len(l1),2):
#    l1[i] = ''
#legend = cax.legend(h1,l1,loc='center right',labelspacing = -.5, fancybox=True, ncol = 5)
#LH = legend.legendHandles
#for i__ in range(len(LH)):
#    if type(LH[i__]) == mpl.lines.Line2D:
#        LH[i__].set_visible(False)
#legend.legendHandles = LH

ax0.axis('off')
#cax.axis('off')
plt.tight_layout()
plt.savefig('data/%s/%s_%s_%s_mst.png' % (project,project,subproject,mode) )
plt.close()



''' Plot the data or a UMAP 2D embed'''
if data.shape[1] == 2:
    plt.figure(dpi=300)
    ''' color clusters'''
    unique = np.unique(key)
    for i in unique:
        idx = key == i
        plt.scatter(data[idx,0],data[idx,1],c=['red','blue'][i],s=2, marker=['o','x'][i])
#    plt.legend()
    plt.xlabel('X'), plt.ylabel('Y'),plt.tight_layout()
    plt.savefig('data/%s/%s/plot.png' % (project,subproject) )
    plt.close()


elif data.shape[1] == 3:
    from mpl_toolkits.mplot3d import Axes3D
    fig = plt.figure(figsize=(4,4),dpi=300)
    ax = Axes3D(fig)
    unique = np.unique(key)
    for i in unique:
        idx = key == i
        ax.scatter(data[idx,0], data[idx,1], data[idx,2], c=plt.cm.tab20([i]))
#    ax.set_xlabel('X')
#    ax.set_ylabel('Y')
#    ax.set_zlabel('Z')
    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_zticks([])
    plt.tight_layout()
    plt.savefig('data/%s/%s_%s_plot.png' % (project,project,subproject) )
    plt.close()
else:

    if cor is None:
        X_umap = umap.UMAP(n_neighbors=K,
                           min_dist=0.3,
                           metric='correlation').fit_transform(data)
    else:
        X_umap = umap.UMAP(n_neighbors=K,
                            min_dist=0.3,
                            metric='precomputed').fit_transform(distance)

    umap_pos = dict( (x, [X_umap[x,0],X_umap[x,1] ]) for x in range(N))
    unique = np.unique( list(name) )
    if project in bag:
        unique = np.unique (list(key))
    plt.figure(figsize=(4,4),dpi=300)
    ax0 = plt.subplot(111)
    ad =nx.Graph()
    ad.add_nodes_from(range(N))
    for i in range(len(unique)):
        idx = list( np.where(key==unique[i])[0] )
        labels = dict( (x, name[x]) for x in idx)
        nx.draw_networkx(ad, pos=umap_pos,
                                        nodelist=idx,
                                        font_size=1,
                                        node_size = 10,
                                        with_labels=False,
                                        labels=labels,
                                        node_color=plt.cm.tab20([i]),
                                        label =  '%s' % unique[i], ax=ax0,
                                        edgelist=[], alpha=0.75)

#    from mpl_toolkits.axes_grid1 import make_axes_locatable
#    h1, l1 = ax0.get_legend_handles_labels()
#    divider = make_axes_locatable(ax0)
#    cax = divider.append_axes("bottom", size="5%", pad=0.1)
#    cax.legend(h1,l1,loc='center right',labelspacing = 0, ncol=5)
    plt.tight_layout()
    ax0.axis('off')
#    cax.axis('off')
    plt.savefig('data/%s/%s_%s_%s_plot_.png' % (project,project,subproject, mode) )
    plt.close()


eee




''' SPC runs sequentially but every temperatures are ran in parallel'''
results = Parallel(n_jobs=5)(delayed( runz )(S,f_,mcmc,nodenext,J,T[y],q,K) for y in range(k))
''' Extract the revelant data from the big file'''
su = np.array([results[i][0] for i in range(k)])
ma = np.array([results[i][1] for i in range(k)])
G = np.array([results[i][2] for i in range(k)])
del results

''' Compute the Spin-Spin correlation function from the 2pt connectedness'''
G= G/mcmc
G = ((q-1)*G)
G+=1
G=G/q

''' save data in case of crash'''
if mode:
    np.save('data/%s/%s/%s/t.npy' % (project,subproject,mode) , T)
    np.save('data/%s/%s/%s/su.npy' % (project,subproject,mode) , su)
    np.save('data/%s/%s/%s/ma.npy' % (project,subproject,mode) , ma)
    np.save('data/%s/%s/%s/G.npy' % (project,subproject,mode) , G)
else:
    np.save('data/%s/%s/t.npy' % (project,subproject) , T)
    np.save('data/%s/%s/su.npy' % (project,subproject) , su)
    np.save('data/%s/%s/ma.npy' % (project,subproject) , ma)
    np.save('data/%s/%s/G.npy' % (project,subproject) , G)

print('STEP 1: DONE')

if mode:
    T = np.load('data/%s/%s/%s/t.npy' % (project,subproject,mode))
    su = np.load('data/%s/%s/%s/su.npy' % (project,subproject,mode))
    ma = np.load('data/%s/%s/%s/ma.npy' % (project,subproject,mode))
    G = np.load('data/%s/%s/%s/G.npy' % (project,subproject,mode))
else:
    T = np.load('data/%s/%s/t.npy' % (project,subproject))
    su = np.load('data/%s/%s/su.npy' % (project,subproject))
    ma = np.load('data/%s/%s/ma.npy' % (project,subproject))
    G = np.load('data/%s/%s/G.npy' % (project,subproject))



plt.figure(),plt.plot(T, su, c='r'), plt.ylabel(r'$\chi$'),plt.xlabel('T'),plt.tight_layout(), plt.savefig('data/%s/%s_su.png' % (project,subproject) )
plt.figure(), plt.plot(T, ma, c='r'), plt.ylabel(r'$\langle m \rangle$'), plt.xlabel(r'$T$'), plt.tight_layout(), plt.savefig('data/%s/%s_ma.png' % (project,subproject) )
plt.close('all')



############## clustering
''' real noisy signal'''
cluster = np.zeros( (k,N) )

''' uses the spinspin correlation matrix to create the cluster configurations'''
Link = []
for i in range(k):
    cluster[i],link = pclus(nodenext, G[i,:,:])
    Link.append(link)
''' saves the clusters configurations'''    
np.save('data/%s/%s_%s_%s_cluster.npy' % (project, project,subproject,mode),cluster)

clus_size = Parallel(n_jobs=-1)(delayed(sclus)(cluster[y],nbr_clus) for y in range(k))
cluster_size = np.zeros( (k, nbr_clus))
for i in range(k):
    cluster_size[i,:] = clus_size[i]

''' create a thermal graph of the clusters sizes in the data'''
plt.figure()
for i in range(nbr_clus):
    plt.plot(T, cluster_size[:, i],label=" cluster %s" % (nbr_clus-i) )
plt.ylabel('Size')
plt.xlabel(r'$T$')
plt.legend()
plt.tight_layout()
plt.savefig('data/%s/%s_%s_%s_cluster.png' % (project,project,subproject, mode) )
plt.close()

''' compute the adjusted rand index when a 'true' solution is known'''
ari = list( map ( lambda x: metrics.adjusted_rand_score(key,x), cluster))


ari_ = ari[i]



''' 3 plots: clusters size, magnetization, and susceptibility'''
''' clusters sizes vs temperature'''
plt.figure(dpi=300)
for r in range(nbr_clus):
    plt.plot(T, cluster_size[:, r], label='cluster %s' % (r+1))
plt.xlabel(r'$T$')
plt.ylabel('size')
plt.axvline(x=T[i], c='red')
#''' legend'''
#plt.legend(loc=4)

''' susceptibility'''
plt.axes([0.7, 0.7, .25, .25])
plt.plot(T, su, c='k')
plt.axvline(x=T[i], c='red')
plt.ylabel(r'$\chi$')
#plt.xlabel('T')
plt.xticks([])
'''magnetization'''
#plt.axes([0.7, 0.34, .25, .25])
plt.axes([0.7, 0.44, .25, .25])
plt.plot(T, ma, c='k')
plt.axvline(x=T[i], c='red')
plt.ylabel(r'$\langle M \rangle $')
plt.xlabel('T')
plt.tight_layout()
plt.savefig('data/%s/%s_%s_%s_clus_ma_su_%s.png' % (project,project,subproject,mode,np.around(T[i], 3)))
plt.close()
######################
''' the minimal spanning tree of the spc solutions, the method builts on the spin-spin
correlation matrix, so why not build the mst and use spin spin correlations as the weight of the
graph?'''

from mpl_toolkits.axes_grid1 import make_axes_locatable
ad = nx.Graph()

''' use the spin spin correlations as the weight of the graph'''
classes = np.unique(cluster[i])
for clas in classes:
    idx = duplicates(cluster[i],clas)
    for node in idx:
        edge_list = [(node,x,1 - G[i,node,x]) for x in idx if x!=node]
        ad.add_weighted_edges_from(edge_list)
''' use the distances of the euclidean matrix / the pearson correlation matrix as the weight
of the graph'''        
#for u in range(N):
##    ad.add_node(u)
#    for v in range(N):
#        if cluster[i,u] == cluster[i,v]:
#            ad.add_edge(u, v, weight=distance[u,v])

Tree=nx.minimum_spanning_tree(ad)
embed = nx.nx_pydot.pydot_layout(Tree)
#embed = nx.nx_pydot.pydot_layout(Tree, prog='sfdp', overlap='compress')
unique = np.unique( list(name) )
if project in bag:
    unique = np.unique (list(key))
plt.figure(figsize=(4,4),dpi=300)
ax0 = plt.subplot(111)
for i____ in range(len(unique)):
    idx = duplicates(key,unique[i____])
    labels = dict( (x, name[x]) for x in idx)
    nx.draw_networkx(Tree, pos=embed,
                                    nodelist=idx,
                                    font_size=1,
                                    node_size = 10,
                                    with_labels=False,
                                    labels=labels,
                                    node_color=plt.cm.tab20([i____]),
                                    label =  '%s' % unique[i____], ax=ax0)

#h1, l1 = ax0.get_legend_handles_labels()
#divider = make_axes_locatable(ax0)
#cax = divider.append_axes("bottom", size="5%", pad=0.1)
#for i0 in range(1,len(l1),2):
#    l1[i0] = ''
#legend = cax.legend(h1,l1,loc='center',labelspacing = -.5, ncol =5)
#LH = legend.legendHandles
#for i__ in range(len(LH)):
#    if type(LH[i__]) == mpl.lines.Line2D:
#        LH[i__].set_visible(False)
#legend.legendHandles = LH
ax0.axis('off')
#cax.axis('off')
plt.tight_layout()
plt.savefig('data/%s/%s_spc_%s_%s_ari_%s.png' % (project,project, mode, np.around(T[i], 3),np.around(ari_,3)))
plt.close()


''' convert spc cluster solutions into gephi graphs'''
# convert digits to strings
g_ = G[i,:,:]
g_[g_<0.5] = 0
ad = nx.from_numpy_matrix(g_)
for o in range(N):
    ad.add_node(o)
    er = np.argmax(G[i,o,nodenext[o]])
    ad.add_edge(o,nodenext[o][er])

spc_class = dict(zip(range(N), cluster[i].astype(str).tolist()))
true_class = dict(zip(range(N), key.astype(str).tolist()))
name_attr = dict(zip(range(N), name.astype(str).tolist()))
nx.set_node_attributes(ad, values = spc_class, name = 'spc_class')
nx.set_node_attributes(ad, name = 'true_class', values = true_class)
nx.set_node_attributes(ad, name = 'name', values = name_attr)
nx.write_gexf(ad,path = 'data/%s/graph_%s_%s_%s_%s_fg.gexf' % (project,project,subproject,mode, np.around(T[i], 3)))

ad = nx.Graph()
distance[distance==0]=1e-4
classes = np.unique(cluster[i])
for clas in classes:
    idx = duplicates(cluster[i],clas)
    for node in idx:
#            edge_list = [(node,x,1 - G[i,node,x]) for x in idx if x!=node]
        edge_list = [(node,x,distance[node,x]) for x in idx if x!=node]
        ad.add_weighted_edges_from(edge_list)

# add attributes:
spc_class = dict(zip(range(N), cluster[i].astype(str).tolist()))
true_class = dict(zip(range(N), key.astype(str).tolist()))
name_attr = dict(zip(range(N), name.astype(str).tolist()))
nx.set_node_attributes(ad, values = spc_class, name = 'spc_class')
nx.set_node_attributes(ad, name = 'true_class', values = true_class)
nx.set_node_attributes(ad, name = 'name', values = name_attr)
nx.write_gexf(ad,path = 'data/%s/graph_%s_%s_%s_%s_hk.gexf' % (project,project,subproject,mode, np.around(T[i], 3)))

'''code for bins of spin spin correlation matrices'''
a = G[2,:,:]
plt.figure(figsize=(4,4),dpi=300)
plt.hist(a, bins='auto', facecolor='red')  # arguments are passed to np.histogram
plt.tight_layout()
plt.savefig('data/%s/rho_g_bins.png' % project )
plt.close()
